{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "# from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import zlib\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image as show_image\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.measure import regionprops_table, label, regionprops\n",
    "from pycocotools import _mask as coco_mask\n",
    "import gc\n",
    "import warnings\n",
    "import torchvision.transforms as T\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import cv2\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class model_config:\n",
    "    current_fold = 0\n",
    "    key = \"BT\" #\"MoCoV2\"\n",
    "    pretrained_resnet = False\n",
    "    seed = 42\n",
    "    train_batch_size = 8\n",
    "    valid_batch_size = 16\n",
    "    epochs = 200  # ~15 minutes per epoch\n",
    "    learning_rate = 0.0014 # 0.001 for bs=16\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    num_training_samples = 5499\n",
    "    T_max = int(\n",
    "        num_training_samples / train_batch_size * epochs)  # number of iterations for a full cycle, need to change for different # of iterations. (iteration = batch size)\n",
    "    weight_decay = 1e-6  # explore different weight decay (Adam optimizer)\n",
    "    n_accumulate = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    iters_to_accumulate = max(1, 32 // train_batch_size)  # for scaling accumulated gradients\n",
    "    eta_min = 1e-5\n",
    "    model_save_directory = os.path.join(os.getcwd(), \"model\",\n",
    "                                        \"seresnext_attention_dropout_dilation_baseline\")  #assuming os.getcwd is the current training script directory\n",
    "\n",
    "# sets the seed of the entire notebook so results are the same every time we run for reproducibility. no randomness, everything is controlled.\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)  #numpy specific random\n",
    "    random.seed(seed)  # python specific random (also for albumentation augmentations)\n",
    "    torch.manual_seed(seed)  # torch specific random\n",
    "    torch.cuda.manual_seed(seed)  # cuda specific random\n",
    "    # when running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # when deterministic = true, benchmark = False, otherwise might not be deterministic\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # set a fixed value for the hash seed, for hases like dictionary\n",
    "\n",
    "set_seed(model_config.seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
